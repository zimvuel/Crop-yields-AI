{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox0t37doj0PJ",
        "outputId": "874d508d-594f-4502-f02f-2da5befe32b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 2023 data...\n",
            "Processing 2024 data...\n",
            "Combining and filtering dates...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1020142287.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Provinsi'] = df['Provinsi'].astype(str).str.strip()\n",
            "/tmp/ipython-input-1020142287.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Provinsi'] = df['Provinsi'].astype(str).str.strip()\n",
            "/tmp/ipython-input-1020142287.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Provinsi'] = df['Provinsi'].astype(str).str.strip()\n",
            "/tmp/ipython-input-1020142287.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Provinsi'] = df['Provinsi'].astype(str).str.strip()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Data saved to /content/Biofarmaka_Daily_Disaggregated_Nov23_Dec24.csv\n",
            "Unique Provinces: 38\n",
            "Provinces found: ['Aceh' 'Bali' 'Banten' 'Bengkulu' 'DI Yogyakarta' 'DKI Jakarta'\n",
            " 'Gorontalo' 'Jambi' 'Jawa Barat' 'Jawa Tengah' 'Jawa Timur'\n",
            " 'Kalimantan Barat' 'Kalimantan Selatan' 'Kalimantan Tengah'\n",
            " 'Kalimantan Timur' 'Kalimantan Utara' 'Kepulauan Bangka Belitung'\n",
            " 'Kepulauan Riau' 'Lampung' 'Maluku' 'Maluku Utara' 'Nusa Tenggara Barat'\n",
            " 'Nusa Tenggara Timur' 'Papua' 'Papua Barat' 'Papua Barat Daya'\n",
            " 'Papua Pegunungan' 'Papua Selatan' 'Papua Tengah' 'Riau' 'Sulawesi Barat'\n",
            " 'Sulawesi Selatan' 'Sulawesi Tengah' 'Sulawesi Tenggara' 'Sulawesi Utara'\n",
            " 'Sumatera Barat' 'Sumatera Selatan' 'Sumatera Utara']\n",
            "Total Rows: 243390\n",
            "        Date Provinsi Jenis Tanaman   Luas Panen      Produksi\n",
            "0 2023-11-01     Aceh          Jahe  3796.813669  11123.005344\n",
            "1 2023-11-01     Aceh   Jeruk Nipis    51.659819    239.176186\n",
            "2 2023-11-01     Aceh      Kapulaga     7.418483     11.579233\n",
            "3 2023-11-01     Aceh        Kencur   137.550491    316.625076\n",
            "4 2023-11-01     Aceh        Kunyit   878.151688   6328.627944\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "\n",
        "def clean_plant_name(col_name):\n",
        "    \"\"\"\n",
        "    Extracts the plant name from the column headers.\n",
        "    Examples:\n",
        "    'Luas Panen Jahe (meter persegi) (M2)' -> 'Jahe'\n",
        "    'Produksi Pisang (Ton)' -> 'Pisang'\n",
        "    \"\"\"\n",
        "    # Remove common prefixes\n",
        "    name = re.sub(r'^(Luas Panen|Produksi)\\s+', '', col_name, flags=re.IGNORECASE)\n",
        "    # Remove units in parentheses (e.g., (M2), (Kg), (pohon))\n",
        "    name = re.sub(r'\\s*\\(.*?\\)', '', name)\n",
        "    return name.strip()\n",
        "\n",
        "def load_and_prep(lp_path, prod_path, year):\n",
        "    \"\"\"\n",
        "    Loads LP (Harvest Area) and Prod (Production) files for a given year,\n",
        "    cleans them, and merges them into a single annual dataframe.\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df_lp = pd.read_csv(lp_path)\n",
        "    df_prod = pd.read_csv(prod_path)\n",
        "\n",
        "    # --- FILTER 1: Drop 'Keji Beling' Columns ---\n",
        "    df_lp = df_lp.loc[:, ~df_lp.columns.str.contains('Keji Beling', case=False)]\n",
        "    df_prod = df_prod.loc[:, ~df_prod.columns.str.contains('Keji Beling', case=False)]\n",
        "\n",
        "    # --- FILTER 2: Strict Province Cleaning ---\n",
        "    def clean_prov_data(df):\n",
        "        if 'Provinsi' not in df.columns:\n",
        "            return df\n",
        "\n",
        "        # 1. Drop rows where Provinsi is NaN\n",
        "        df = df.dropna(subset=['Provinsi'])\n",
        "\n",
        "        # 2. Convert to string and strip whitespace\n",
        "        df['Provinsi'] = df['Provinsi'].astype(str).str.strip()\n",
        "\n",
        "        # 3. Define invalid values (Exact matches to remove)\n",
        "        invalid_exact = ['nan', '0', '', 'indonesia']\n",
        "\n",
        "        # 4. Define text patterns to remove (Rows containing these words)\n",
        "        # BPS files often have footers like \"Angka Tetap\", \"Angka Sementara\", \"Sumber:\", \"Catatan:\"\n",
        "        blacklist_keywords = [\n",
        "            'catatan',\n",
        "            'angka tetap',\n",
        "            'angka sementara',\n",
        "            'angka sangat sementara',\n",
        "            'sumber',\n",
        "            'jumlah',\n",
        "            'total'\n",
        "        ]\n",
        "\n",
        "        # Filter logic\n",
        "        # Start with rows that are NOT in invalid_exact\n",
        "        mask = ~df['Provinsi'].str.lower().isin(invalid_exact)\n",
        "\n",
        "        # Loop through blacklist and remove any row containing these words\n",
        "        for kw in blacklist_keywords:\n",
        "            mask = mask & ~df['Provinsi'].str.contains(kw, case=False, na=False)\n",
        "\n",
        "        # Also remove rows that are purely numbers (like row numbers '1', '2', etc.)\n",
        "        mask = mask & ~df['Provinsi'].str.match(r'^\\d+$')\n",
        "\n",
        "        return df[mask]\n",
        "\n",
        "    df_lp = clean_prov_data(df_lp)\n",
        "    df_prod = clean_prov_data(df_prod)\n",
        "\n",
        "    # Clean columns mapping\n",
        "    # Keep 'Provinsi' as is, rename others using regex\n",
        "    lp_cols = {c: clean_plant_name(c) for c in df_lp.columns if c != 'Provinsi'}\n",
        "    prod_cols = {c: clean_plant_name(c) for c in df_prod.columns if c != 'Provinsi'}\n",
        "\n",
        "    df_lp.rename(columns=lp_cols, inplace=True)\n",
        "    df_prod.rename(columns=prod_cols, inplace=True)\n",
        "\n",
        "    # Melt to long format\n",
        "    lp_melt = df_lp.melt(id_vars=['Provinsi'], var_name='Jenis Tanaman', value_name='Luas Panen')\n",
        "    prod_melt = df_prod.melt(id_vars=['Provinsi'], var_name='Jenis Tanaman', value_name='Produksi')\n",
        "\n",
        "    # Clean non-numeric values\n",
        "    def clean_numeric(x):\n",
        "        try:\n",
        "            return float(x)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    lp_melt['Luas Panen'] = lp_melt['Luas Panen'].apply(clean_numeric)\n",
        "    prod_melt['Produksi'] = prod_melt['Produksi'].apply(clean_numeric)\n",
        "\n",
        "    # Merge\n",
        "    df_annual = pd.merge(lp_melt, prod_melt, on=['Provinsi', 'Jenis Tanaman'], how='outer')\n",
        "    df_annual['Year'] = year\n",
        "\n",
        "    # Fill NaNs with 0 (Only affects numeric columns usually, but good for safety)\n",
        "    df_annual.fillna(0, inplace=True)\n",
        "\n",
        "    return df_annual\n",
        "\n",
        "def expand_to_daily(df_annual, year):\n",
        "    \"\"\"\n",
        "    Expands annual data into daily data with randomized distribution.\n",
        "    Ensures sum of days equals annual total.\n",
        "    \"\"\"\n",
        "    start_date = f'{year}-01-01'\n",
        "    end_date = f'{year}-12-31'\n",
        "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "    days_count = len(dates)\n",
        "\n",
        "    # Create a scaffold dataframe with every date for every row in annual data\n",
        "    n_rows = len(df_annual)\n",
        "\n",
        "    # Repeat indices\n",
        "    repeated_indices = np.repeat(df_annual.index, days_count)\n",
        "\n",
        "    # Create the daily dataframe\n",
        "    df_daily = df_annual.loc[repeated_indices].copy()\n",
        "\n",
        "    # Assign dates\n",
        "    df_daily['Date'] = np.tile(dates, n_rows)\n",
        "\n",
        "    # --- Randomization Logic ---\n",
        "    np.random.seed(42 + year)\n",
        "    noise_lp = np.random.uniform(0.2, 1.8, size=len(df_daily))\n",
        "    noise_prod = np.random.uniform(0.2, 1.8, size=len(df_daily))\n",
        "\n",
        "    df_daily['weight_lp'] = noise_lp\n",
        "    df_daily['weight_prod'] = noise_prod\n",
        "\n",
        "    # Calculate sum of weights per original row\n",
        "    weight_sums = df_daily.groupby(df_daily.index)[['weight_lp', 'weight_prod']].transform('sum')\n",
        "\n",
        "    # Calculate Final Daily Values\n",
        "    df_daily['Luas Panen Daily'] = df_daily['Luas Panen'] * (df_daily['weight_lp'] / weight_sums['weight_lp'])\n",
        "    df_daily['Produksi Daily'] = df_daily['Produksi'] * (df_daily['weight_prod'] / weight_sums['weight_prod'])\n",
        "\n",
        "    # Cleanup\n",
        "    final_cols = ['Date', 'Provinsi', 'Jenis Tanaman', 'Luas Panen Daily', 'Produksi Daily']\n",
        "    df_daily = df_daily[final_cols]\n",
        "    df_daily.rename(columns={'Luas Panen Daily': 'Luas Panen', 'Produksi Daily': 'Produksi'}, inplace=True)\n",
        "\n",
        "    return df_daily\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "# Determine the directory where this script is located\n",
        "try:\n",
        "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "except NameError:\n",
        "    script_dir = os.getcwd()\n",
        "\n",
        "# File Paths\n",
        "files = {\n",
        "    '2023': {\n",
        "        'lp': os.path.join(script_dir, 'Luas Panen Tanaman Biofarmaka Menurut Provinsi dan Jenis Tanaman, 2023.csv'),\n",
        "        'prod': os.path.join(script_dir, 'Produksi Tanaman Biofarmaka Menurut Provinsi dan Jenis Tanaman, 2023.csv')\n",
        "    },\n",
        "    '2024': {\n",
        "        'lp': os.path.join(script_dir, 'Luas Panen Tanaman Biofarmaka Menurut Provinsi dan Jenis TanamanÂ , 2024.csv'),\n",
        "        'prod': os.path.join(script_dir, 'Produksi Tanaman Biofarmaka Menurut Provinsi dan Jenis TanamanÂ , 2024.csv')\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Processing 2023 data...\")\n",
        "df_2023_annual = load_and_prep(files['2023']['lp'], files['2023']['prod'], 2023)\n",
        "df_2023_daily = expand_to_daily(df_2023_annual, 2023)\n",
        "\n",
        "print(\"Processing 2024 data...\")\n",
        "df_2024_annual = load_and_prep(files['2024']['lp'], files['2024']['prod'], 2024)\n",
        "df_2024_daily = expand_to_daily(df_2024_annual, 2024)\n",
        "\n",
        "# Combine\n",
        "print(\"Combining and filtering dates...\")\n",
        "df_combined = pd.concat([df_2023_daily, df_2024_daily], axis=0)\n",
        "\n",
        "# Filter for requested range: Nov 1 2023 to Dec 31 2024\n",
        "start_filter = pd.Timestamp('2023-11-01')\n",
        "end_filter = pd.Timestamp('2024-12-31')\n",
        "\n",
        "df_final = df_combined[(df_combined['Date'] >= start_filter) & (df_combined['Date'] <= end_filter)]\n",
        "\n",
        "# Sorting\n",
        "df_final = df_final.sort_values(by=['Date', 'Provinsi', 'Jenis Tanaman'])\n",
        "\n",
        "# Save to CSV\n",
        "output_filename = os.path.join(script_dir, 'Biofarmaka_Daily_Disaggregated_Nov23_Dec24.csv')\n",
        "df_final.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Success! Data saved to {output_filename}\")\n",
        "print(f\"Unique Provinces: {df_final['Provinsi'].nunique()}\")\n",
        "print(\"Provinces found:\", df_final['Provinsi'].unique())\n",
        "print(f\"Total Rows: {len(df_final)}\")\n",
        "print(df_final.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkFmW_aluSC5",
        "outputId": "13d29225-f625-4b2e-cbea-930e5523191f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ]
    }
  ]
}