{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox0t37doj0PJ",
        "outputId": "81662a14-9cc3-4a20-9f68-10617211a503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/Daily_Padi_2025.csv...\n",
            "  - Disaggregating Luas Panen...\n",
            "  - Disaggregating Produksi...\n",
            "  - Merging datasets...\n",
            "✅ Saved to /content/Daily_Padi_2025.csv\n",
            "Processing /content/Daily_Jagung_2025.csv...\n",
            "  - Disaggregating Luas Panen...\n",
            "  - Disaggregating Produksi...\n",
            "  - Merging datasets...\n",
            "✅ Saved to /content/Daily_Jagung_2025.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import calendar\n",
        "import re\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "YEAR = 2025\n",
        "\n",
        "# Mapping Indonesian Month names to integers\n",
        "MONTH_MAP = {\n",
        "    'Januari': 1, 'Februari': 2, 'Maret': 3, 'April': 4, 'Mei': 5, 'Juni': 6,\n",
        "    'Juli': 7, 'Agustus': 8, 'September': 9, 'Oktober': 10, 'November': 11, 'Desember': 12\n",
        "}\n",
        "\n",
        "def clean_province_name(prov):\n",
        "    \"\"\"Standardizes province names to ensure merging works.\"\"\"\n",
        "    if pd.isna(prov): return \"\"\n",
        "    prov = str(prov).strip().upper()\n",
        "    return prov\n",
        "\n",
        "def load_and_melt(filepath, value_name):\n",
        "    \"\"\"\n",
        "    Loads the BPS specific format (header on row 3), cleans it,\n",
        "    and melts it from Wide (Months) to Long (Rows).\n",
        "    \"\"\"\n",
        "    # Read CSV, skipping first 3 metadata rows\n",
        "    df = pd.read_csv(filepath, header=3)\n",
        "\n",
        "    # Rename first column (usually unnamed or index) to 'Provinsi'\n",
        "    df.columns.values[0] = 'Provinsi'\n",
        "\n",
        "    # Filter rows: Remove 'INDONESIA', 'Tahunan', empty rows, or purely numeric rows\n",
        "    df['Provinsi'] = df['Provinsi'].apply(clean_province_name)\n",
        "    df = df[df['Provinsi'] != '']\n",
        "    df = df[df['Provinsi'] != 'INDONESIA']\n",
        "    df = df[~df['Provinsi'].str.contains('PROVINSI', case=False)] # Header repeats\n",
        "\n",
        "    # Drop 'Tahunan' column if exists\n",
        "    if 'Tahunan' in df.columns:\n",
        "        df = df.drop(columns=['Tahunan'])\n",
        "\n",
        "    # Melt (Columns Jan-Dec becomes rows)\n",
        "    # Filter columns to only include keys in MONTH_MAP or 'Provinsi'\n",
        "    valid_cols = ['Provinsi'] + [col for col in df.columns if col in MONTH_MAP]\n",
        "    df = df[valid_cols]\n",
        "\n",
        "    df_long = df.melt(id_vars=['Provinsi'], var_name='Bulan', value_name=value_name)\n",
        "\n",
        "    # Map month names to numbers\n",
        "    df_long['Month_Num'] = df_long['Bulan'].map(MONTH_MAP)\n",
        "\n",
        "    # Clean numeric values\n",
        "    def clean_num(x):\n",
        "        try:\n",
        "            return float(x)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    df_long[value_name] = df_long[value_name].apply(clean_num)\n",
        "\n",
        "    return df_long\n",
        "\n",
        "def disaggregate_to_daily(row):\n",
        "    \"\"\"\n",
        "    Takes a monthly row and returns a DataFrame of daily rows.\n",
        "    \"\"\"\n",
        "    prov = row['Provinsi']\n",
        "    month = int(row['Month_Num'])\n",
        "    val = row['Value'] # This will be either Luas Panen or Produksi\n",
        "    col_name = row['Type'] # 'Luas Panen' or 'Produksi'\n",
        "\n",
        "    # Get number of days in this specific month/year\n",
        "    # monthrange returns (weekday_of_first_day, number_of_days)\n",
        "    _, num_days = calendar.monthrange(YEAR, month)\n",
        "\n",
        "    # Generate Date Range\n",
        "    start_date = f\"{YEAR}-{month:02d}-01\"\n",
        "    dates = pd.date_range(start=start_date, periods=num_days, freq='D')\n",
        "\n",
        "    # --- Randomization Logic ---\n",
        "    # Generate random weights\n",
        "    np.random.seed(hash(prov) % 10000 + month + int(val)) # Deterministic seed per row\n",
        "    weights = np.random.uniform(0.3, 1.7, size=num_days)\n",
        "\n",
        "    # Normalize weights so they sum to exactly 1.0\n",
        "    normalized_weights = weights / weights.sum()\n",
        "\n",
        "    # Calculate daily values\n",
        "    daily_values = val * normalized_weights\n",
        "\n",
        "    # Create DataFrame\n",
        "    temp_df = pd.DataFrame({\n",
        "        'Date': dates,\n",
        "        'Provinsi': prov,\n",
        "        col_name: daily_values\n",
        "    })\n",
        "\n",
        "    return temp_df\n",
        "\n",
        "def process_commodity(lp_file, prod_file, output_name):\n",
        "    print(f\"Processing {output_name}...\")\n",
        "\n",
        "    # 1. Load and Melt\n",
        "    df_lp = load_and_melt(lp_file, 'Luas Panen')\n",
        "    df_prod = load_and_melt(prod_file, 'Produksi')\n",
        "\n",
        "    # 2. Expand Luas Panen to Daily\n",
        "    print(f\"  - Disaggregating Luas Panen...\")\n",
        "    lp_daily_list = []\n",
        "    # Prepare row for function\n",
        "    df_lp['Type'] = 'Luas Panen'\n",
        "    df_lp.rename(columns={'Luas Panen': 'Value'}, inplace=True)\n",
        "\n",
        "    for _, row in df_lp.iterrows():\n",
        "        lp_daily_list.append(disaggregate_to_daily(row))\n",
        "\n",
        "    df_lp_daily = pd.concat(lp_daily_list, ignore_index=True)\n",
        "\n",
        "    # 3. Expand Produksi to Daily\n",
        "    print(f\"  - Disaggregating Produksi...\")\n",
        "    prod_daily_list = []\n",
        "    df_prod['Type'] = 'Produksi'\n",
        "    df_prod.rename(columns={'Produksi': 'Value'}, inplace=True)\n",
        "\n",
        "    for _, row in df_prod.iterrows():\n",
        "        prod_daily_list.append(disaggregate_to_daily(row))\n",
        "\n",
        "    df_prod_daily = pd.concat(prod_daily_list, ignore_index=True)\n",
        "\n",
        "    # 4. Merge\n",
        "    print(f\"  - Merging datasets...\")\n",
        "    # Merge on Date and Provinsi\n",
        "    df_final = pd.merge(df_lp_daily, df_prod_daily, on=['Date', 'Provinsi'], how='outer')\n",
        "\n",
        "    # Fill NaNs with 0\n",
        "    df_final.fillna(0, inplace=True)\n",
        "\n",
        "    # Sort\n",
        "    df_final = df_final.sort_values(by=['Date', 'Provinsi'])\n",
        "\n",
        "    # Save\n",
        "    df_final.to_csv(output_name, index=False)\n",
        "    print(f\"✅ Saved to {output_name}\")\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "\n",
        "# Get Script Directory\n",
        "try:\n",
        "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "except NameError:\n",
        "    script_dir = os.getcwd()\n",
        "\n",
        "# Define File Pairs\n",
        "files = {\n",
        "    'Padi': {\n",
        "        'lp': 'Luas Panen Padi Menurut Provinsi (Bulanan), 2025.csv',\n",
        "        'prod': 'Produksi Padi Menurut Provinsi (Bulanan), 2025.csv',\n",
        "        'out': 'Daily_Padi_2025.csv'\n",
        "    },\n",
        "    'Jagung': {\n",
        "        'lp': 'Luas Panen Jagung Pipilan Menurut Provinsi (Bulanan), 2025.csv',\n",
        "        'prod': 'Produksi Jagung Pipilan Kering Kadar Air 14 Persen Menurut Provinsi (Bulanan), 2025.csv',\n",
        "        'out': 'Daily_Jagung_2025.csv'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Run Process\n",
        "for commodity, paths in files.items():\n",
        "    lp_path = os.path.join(script_dir, paths['lp'])\n",
        "    prod_path = os.path.join(script_dir, paths['prod'])\n",
        "    out_path = os.path.join(script_dir, paths['out'])\n",
        "\n",
        "    if os.path.exists(lp_path) and os.path.exists(prod_path):\n",
        "        process_commodity(lp_path, prod_path, out_path)\n",
        "    else:\n",
        "        print(f\"❌ Files for {commodity} not found in {script_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import calendar\n",
        "import re\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# We will process these years\n",
        "YEARS_TO_PROCESS = [2023, 2024]\n",
        "REFERENCE_YEAR = 2025  # Used for seasonality weights\n",
        "\n",
        "# Mapping Indonesian Month names to integers\n",
        "MONTH_MAP = {\n",
        "    'Januari': 1, 'Februari': 2, 'Maret': 3, 'April': 4, 'Mei': 5, 'Juni': 6,\n",
        "    'Juli': 7, 'Agustus': 8, 'September': 9, 'Oktober': 10, 'November': 11, 'Desember': 12\n",
        "}\n",
        "\n",
        "def clean_province_name(prov):\n",
        "    \"\"\"Standardizes province names to ensure merging works.\"\"\"\n",
        "    if pd.isna(prov): return \"\"\n",
        "    prov = str(prov).strip().upper()\n",
        "    return prov\n",
        "\n",
        "def load_seasonality_weights(filepath):\n",
        "    \"\"\"\n",
        "    Loads 2025 Monthly data to calculate the proportion of harvest/production\n",
        "    per month for each province.\n",
        "    \"\"\"\n",
        "    # Read CSV, skipping first 3 metadata rows (BPS format specific to 2025 files)\n",
        "    df = pd.read_csv(filepath, header=3)\n",
        "\n",
        "    # Rename first column\n",
        "    df.columns.values[0] = 'Provinsi'\n",
        "\n",
        "    # Filter valid provinces\n",
        "    df['Provinsi'] = df['Provinsi'].apply(clean_province_name)\n",
        "    df = df[df['Provinsi'] != '']\n",
        "    df = df[df['Provinsi'] != 'INDONESIA']\n",
        "    df = df[~df['Provinsi'].str.contains('PROVINSI', case=False)]\n",
        "\n",
        "    if 'Tahunan' in df.columns:\n",
        "        df = df.drop(columns=['Tahunan'])\n",
        "\n",
        "    # Keep only month columns and Provinsi\n",
        "    valid_cols = ['Provinsi'] + [col for col in df.columns if col in MONTH_MAP]\n",
        "    df = df[valid_cols]\n",
        "\n",
        "    # Clean numeric values\n",
        "    for col in valid_cols[1:]:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # Set Index\n",
        "    df.set_index('Provinsi', inplace=True)\n",
        "\n",
        "    # Calculate Weights (Row-wise normalization)\n",
        "    row_sums = df.sum(axis=1)\n",
        "    weights = df.div(row_sums, axis=0)\n",
        "\n",
        "    # Handle NaN weights (if total was 0, use uniform distribution)\n",
        "    weights.fillna(1/12, inplace=True)\n",
        "\n",
        "    # Rename columns to 1..12\n",
        "    weights.columns = [MONTH_MAP[c] for c in weights.columns]\n",
        "\n",
        "    return weights\n",
        "\n",
        "def load_annual_data(filepath):\n",
        "    \"\"\"\n",
        "    Loads the 2023/2024 Annual Combined files.\n",
        "    Format is typically header at row 2 (index 2).\n",
        "    Cols: Provinsi, Luas Panen, Produktivitas, Produksi\n",
        "    \"\"\"\n",
        "    # Read with header at index 2 (Row 3 in Excel/CSV)\n",
        "    df = pd.read_csv(filepath, header=2)\n",
        "\n",
        "    # The row immediately after header often contains years \"2023, 2023...\", drop it\n",
        "    if str(df.iloc[0, 1]).strip() in ['2023', '2024'] or pd.isna(df.iloc[0, 0]):\n",
        "        df = df.iloc[1:].reset_index(drop=True)\n",
        "\n",
        "    # Explicitly Select and Rename Columns\n",
        "    # Index 0: Provinsi\n",
        "    # Index 1: Luas Panen\n",
        "    # Index 2: Produktivitas (DROPPED/IGNORED)\n",
        "    # Index 3: Produksi\n",
        "\n",
        "    # We rename only the ones we need\n",
        "    df.rename(columns={\n",
        "        df.columns[0]: 'Provinsi',\n",
        "        df.columns[1]: 'Luas Panen',\n",
        "        df.columns[3]: 'Produksi'\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Filter only these 3 columns (Dropping Produktivitas here)\n",
        "    df = df[['Provinsi', 'Luas Panen', 'Produksi']]\n",
        "\n",
        "    # Clean Province\n",
        "    df['Provinsi'] = df['Provinsi'].apply(clean_province_name)\n",
        "    df = df[df['Provinsi'] != '']\n",
        "    df = df[~df['Provinsi'].str.contains('PROVINSI', case=False)]\n",
        "\n",
        "    # Clean Numerics\n",
        "    for col in ['Luas Panen', 'Produksi']:\n",
        "        if df[col].dtype == object:\n",
        "             df[col] = df[col].astype(str).str.replace(',', '').replace('-', '0')\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    return df\n",
        "\n",
        "def generate_daily_from_annual_row(row, year, lp_weights, prod_weights):\n",
        "    \"\"\"\n",
        "    Disaggregates a single annual row into 365/366 daily rows.\n",
        "    \"\"\"\n",
        "    prov = row['Provinsi']\n",
        "    annual_lp = row['Luas Panen']\n",
        "    annual_prod = row['Produksi']\n",
        "\n",
        "    # Get Weights for this province (default to uniform if missing)\n",
        "    if prov in lp_weights.index:\n",
        "        w_lp = lp_weights.loc[prov].values\n",
        "    else:\n",
        "        w_lp = np.full(12, 1/12)\n",
        "\n",
        "    if prov in prod_weights.index:\n",
        "        w_prod = prod_weights.loc[prov].values\n",
        "    else:\n",
        "        w_prod = np.full(12, 1/12)\n",
        "\n",
        "    # --- Weight Jittering (The \"Small Role\" Logic) ---\n",
        "    # We perturb the 2025 weights slightly so 2023/24 aren't exact copies\n",
        "    # Fix: Ensure seed is within valid 32-bit range for numpy\n",
        "    seed_val = (abs(hash(prov)) + year) % (2**32)\n",
        "    np.random.seed(seed_val)\n",
        "\n",
        "    # Add noise: random factor between 0.8 and 1.2\n",
        "    noise_lp = np.random.uniform(0.8, 1.2, size=12)\n",
        "    noise_prod = np.random.uniform(0.8, 1.2, size=12)\n",
        "\n",
        "    # Apply noise and re-normalize\n",
        "    w_lp_adj = w_lp * noise_lp\n",
        "    w_lp_adj = w_lp_adj / w_lp_adj.sum()\n",
        "\n",
        "    w_prod_adj = w_prod * noise_prod\n",
        "    w_prod_adj = w_prod_adj / w_prod_adj.sum()\n",
        "\n",
        "    # --- Generate Daily Data ---\n",
        "    all_dates = []\n",
        "    all_lp = []\n",
        "    all_prod = []\n",
        "\n",
        "    for month_idx in range(12):\n",
        "        month_num = month_idx + 1\n",
        "\n",
        "        # Monthly Total for this year based on adjusted weights\n",
        "        month_lp_total = annual_lp * w_lp_adj[month_idx]\n",
        "        month_prod_total = annual_prod * w_prod_adj[month_idx]\n",
        "\n",
        "        # Days in month (handle leap year for 2024)\n",
        "        _, num_days = calendar.monthrange(year, month_num)\n",
        "\n",
        "        # Daily Weights (Random non-normal distribution within month)\n",
        "        # Using a fresh seed for daily variation\n",
        "        daily_noise = np.random.uniform(0.3, 1.7, size=num_days)\n",
        "        daily_w = daily_noise / daily_noise.sum()\n",
        "\n",
        "        # Calculate daily values\n",
        "        daily_lp_vals = month_lp_total * daily_w\n",
        "        daily_prod_vals = month_prod_total * daily_w\n",
        "\n",
        "        # Generate Dates\n",
        "        start = f\"{year}-{month_num:02d}-01\"\n",
        "        dates = pd.date_range(start=start, periods=num_days, freq='D')\n",
        "\n",
        "        all_dates.extend(dates)\n",
        "        all_lp.extend(daily_lp_vals)\n",
        "        all_prod.extend(daily_prod_vals)\n",
        "\n",
        "    # Create DataFrame\n",
        "    return pd.DataFrame({\n",
        "        'Date': all_dates,\n",
        "        'Provinsi': prov,\n",
        "        'Luas Panen': all_lp,\n",
        "        'Produksi': all_prod\n",
        "    })\n",
        "\n",
        "def process_commodity_year(commodity, year, annual_file, ref_lp_file, ref_prod_file, output_name):\n",
        "    print(f\"Processing {commodity} for {year}...\")\n",
        "\n",
        "    # 1. Load Seasonality Weights (from 2025 Data)\n",
        "    lp_weights_df = load_seasonality_weights(ref_lp_file)\n",
        "    prod_weights_df = load_seasonality_weights(ref_prod_file)\n",
        "\n",
        "    # 2. Load Annual Data\n",
        "    df_annual = load_annual_data(annual_file)\n",
        "\n",
        "    # 3. Process Rows\n",
        "    daily_dfs = []\n",
        "    for _, row in df_annual.iterrows():\n",
        "        daily_dfs.append(generate_daily_from_annual_row(row, year, lp_weights_df, prod_weights_df))\n",
        "\n",
        "    # 4. Combine and Save\n",
        "    if daily_dfs:\n",
        "        df_final = pd.concat(daily_dfs, ignore_index=True)\n",
        "        df_final = df_final.sort_values(by=['Date', 'Provinsi'])\n",
        "        df_final.to_csv(output_name, index=False)\n",
        "        print(f\"✅ Saved to {output_name}\")\n",
        "    else:\n",
        "        print(\"❌ No data processed.\")\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "\n",
        "try:\n",
        "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "except NameError:\n",
        "    script_dir = os.getcwd()\n",
        "\n",
        "# Configuration Dictionary\n",
        "tasks = {\n",
        "    'Padi': {\n",
        "        'ref_lp': 'Luas Panen Padi Menurut Provinsi (Bulanan), 2025.csv',\n",
        "        'ref_prod': 'Produksi Padi Menurut Provinsi (Bulanan), 2025.csv',\n",
        "        'years': {\n",
        "            2023: 'Luas Panen, Produksi, dan Produktivitas Padi Menurut Provinsi, 2023.csv',\n",
        "            2024: 'Luas Panen, Produksi, dan Produktivitas Padi Menurut Provinsi, 2024.csv'\n",
        "        }\n",
        "    },\n",
        "    'Jagung': {\n",
        "        'ref_lp': 'Luas Panen Jagung Pipilan Menurut Provinsi (Bulanan), 2025.csv',\n",
        "        'ref_prod': 'Produksi Jagung Pipilan Kering Kadar Air 14 Persen Menurut Provinsi (Bulanan), 2025.csv',\n",
        "        'years': {\n",
        "            2023: 'Luas Panen, Produksi, dan Produktivitas Jagung Menurut Provinsi, 2023.csv',\n",
        "            2024: 'Luas Panen, Produksi, dan Produktivitas Jagung Menurut Provinsi, 2024.csv'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "for commodity, config in tasks.items():\n",
        "    ref_lp_path = os.path.join(script_dir, config['ref_lp'])\n",
        "    ref_prod_path = os.path.join(script_dir, config['ref_prod'])\n",
        "\n",
        "    # Check if reference files exist\n",
        "    if not (os.path.exists(ref_lp_path) and os.path.exists(ref_prod_path)):\n",
        "        print(f\"⚠️ Reference files for {commodity} (2025) missing. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    for year, filename in config['years'].items():\n",
        "        annual_path = os.path.join(script_dir, filename)\n",
        "        output_filename = f\"Daily_{commodity}_{year}.csv\"\n",
        "        output_path = os.path.join(script_dir, output_filename)\n",
        "\n",
        "        if os.path.exists(annual_path):\n",
        "            process_commodity_year(\n",
        "                commodity,\n",
        "                year,\n",
        "                annual_path,\n",
        "                ref_lp_path,\n",
        "                ref_prod_path,\n",
        "                output_path\n",
        "            )\n",
        "        else:\n",
        "            print(f\"❌ Annual file for {commodity} {year} not found: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klXbghFt24zC",
        "outputId": "e33d7a95-534e-4c71-fb72-62417b98b3b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Padi for 2023...\n",
            "✅ Saved to /content/Daily_Padi_2023.csv\n",
            "Processing Padi for 2024...\n",
            "✅ Saved to /content/Daily_Padi_2024.csv\n",
            "Processing Jagung for 2023...\n",
            "✅ Saved to /content/Daily_Jagung_2023.csv\n",
            "Processing Jagung for 2024...\n",
            "✅ Saved to /content/Daily_Jagung_2024.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "VALID_PROVINCES = {\n",
        "    \"ACEH\", \"SUMATERA UTARA\", \"SUMATERA BARAT\", \"RIAU\", \"JAMBI\",\n",
        "    \"SUMATERA SELATAN\", \"BENGKULU\", \"LAMPUNG\", \"KEP. BANGKA BELITUNG\",\n",
        "    \"KEP. RIAU\", \"DKI JAKARTA\", \"JAWA BARAT\", \"JAWA TENGAH\",\n",
        "    \"DI YOGYAKARTA\", \"JAWA TIMUR\", \"BANTEN\", \"BALI\",\n",
        "    \"NUSA TENGGARA BARAT\", \"NUSA TENGGARA TIMUR\", \"KALIMANTAN BARAT\",\n",
        "    \"KALIMANTAN TENGAH\", \"KALIMANTAN SELATAN\", \"KALIMANTAN TIMUR\",\n",
        "    \"KALIMANTAN UTARA\", \"SULAWESI UTARA\", \"SULAWESI TENGAH\",\n",
        "    \"SULAWESI SELATAN\", \"SULAWESI TENGGARA\", \"GORONTALO\",\n",
        "    \"SULAWESI BARAT\", \"MALUKU\", \"MALUKU UTARA\", \"PAPUA BARAT\",\n",
        "    \"PAPUA\", \"PAPUA SELATAN\", \"PAPUA TENGAH\",\n",
        "    \"PAPUA PEGUNUNGAN\", \"PAPUA BARAT DAYA\"\n",
        "}\n",
        "\n",
        "# Define Date Filter Range\n",
        "START_DATE_FILTER = \"2023-11-01\"\n",
        "END_DATE_FILTER = \"2025-10-31\"\n",
        "\n",
        "def clean_province_name(prov):\n",
        "    if pd.isna(prov):\n",
        "        return \"\"\n",
        "    # Standardize: Uppercase and Strip\n",
        "    return str(prov).strip().upper()\n",
        "\n",
        "def process_commodity_group(commodity_name, file_list, output_filename):\n",
        "    print(f\"\\nProcessing {commodity_name}...\")\n",
        "    dfs = []\n",
        "\n",
        "    # 1. Read and Concatenate Files\n",
        "    for filename in file_list:\n",
        "        if os.path.exists(filename):\n",
        "            print(f\"  - Reading {filename}...\")\n",
        "            df = pd.read_csv(filename)\n",
        "            dfs.append(df)\n",
        "        else:\n",
        "            print(f\"  ❌ Warning: {filename} not found. Skipping.\")\n",
        "\n",
        "    if not dfs:\n",
        "        print(\"  No files found to combine.\")\n",
        "        return\n",
        "\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "    original_count = len(combined_df)\n",
        "\n",
        "    # 2. Clean Province Names\n",
        "    combined_df['Provinsi'] = combined_df['Provinsi'].apply(clean_province_name)\n",
        "\n",
        "    # 3. Filter Logic\n",
        "    # 3a. Date Filter (Nov 1 2023 - Oct 31 2025)\n",
        "    if 'Date' in combined_df.columns:\n",
        "        combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
        "\n",
        "        start_ts = pd.Timestamp(START_DATE_FILTER)\n",
        "        end_ts = pd.Timestamp(END_DATE_FILTER)\n",
        "\n",
        "        # Filter mask\n",
        "        date_mask = (combined_df['Date'] >= start_ts) & (combined_df['Date'] <= end_ts)\n",
        "        combined_df = combined_df[date_mask]\n",
        "\n",
        "    # 3b. Province Filter\n",
        "    # Keep row IF Province is in VALID_PROVINCES\n",
        "    # This automatically drops 'INDONESIA', typos, or header repeats\n",
        "    combined_df = combined_df[combined_df['Provinsi'].isin(VALID_PROVINCES)]\n",
        "\n",
        "    filtered_count = len(combined_df)\n",
        "    dropped_count = original_count - filtered_count\n",
        "\n",
        "    # 4. Sort (Date -> Province)\n",
        "    combined_df = combined_df.sort_values(by=['Date', 'Provinsi'])\n",
        "\n",
        "    # 5. Save\n",
        "    combined_df.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"  ✅ Saved to {output_filename}\")\n",
        "    print(f\"  - Original Rows: {original_count}\")\n",
        "    print(f\"  - Rows After Filtering: {filtered_count} (Dropped {dropped_count})\")\n",
        "    print(f\"  - Unique Provinces: {combined_df['Provinsi'].nunique()}\")\n",
        "    if not combined_df.empty:\n",
        "        print(f\"  - Date Range: {combined_df['Date'].min().date()} to {combined_df['Date'].max().date()}\")\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "\n",
        "# Define the file sets\n",
        "padi_files = [\n",
        "    'Daily_Padi_2023.csv',\n",
        "    'Daily_Padi_2024.csv',\n",
        "    'Daily_Padi_2025.csv'\n",
        "]\n",
        "\n",
        "jagung_files = [\n",
        "    'Daily_Jagung_2023.csv',\n",
        "    'Daily_Jagung_2024.csv',\n",
        "    'Daily_Jagung_2025.csv'\n",
        "]\n",
        "\n",
        "# Run Combination\n",
        "process_commodity_group(\"Padi\", padi_files, \"Combined_Daily_Padi_Nov23_Oct25.csv\")\n",
        "process_commodity_group(\"Jagung\", jagung_files, \"Combined_Daily_Jagung_Nov23_Oct25.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QW7a4FJ4Bwk",
        "outputId": "c6b93c3d-bbf8-4b02-e29f-0519fde9baf3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Padi...\n",
            "  - Reading Daily_Padi_2023.csv...\n",
            "  - Reading Daily_Padi_2024.csv...\n",
            "  - Reading Daily_Padi_2025.csv...\n",
            "  ✅ Saved to Combined_Daily_Padi_Nov23_Oct25.csv\n",
            "  - Original Rows: 42379\n",
            "  - Rows After Filtering: 27778 (Dropped 14601)\n",
            "  - Unique Provinces: 38\n",
            "  - Date Range: 2023-11-01 to 2025-10-31\n",
            "\n",
            "Processing Jagung...\n",
            "  - Reading Daily_Jagung_2023.csv...\n",
            "  - Reading Daily_Jagung_2024.csv...\n",
            "  - Reading Daily_Jagung_2025.csv...\n",
            "  ✅ Saved to Combined_Daily_Jagung_Nov23_Oct25.csv\n",
            "  - Original Rows: 42379\n",
            "  - Rows After Filtering: 27778 (Dropped 14601)\n",
            "  - Unique Provinces: 38\n",
            "  - Date Range: 2023-11-01 to 2025-10-31\n"
          ]
        }
      ]
    }
  ]
}